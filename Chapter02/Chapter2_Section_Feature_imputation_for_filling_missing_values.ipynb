{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN0yRvQw9G9ymemWd+s6yas"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Chapter 2\n","## Section: Feature imputation for filling missing values\n","This notebook helps you to understand how to apply feature imputation using scikit-learn. \\\\\n","The following code focuses on feature imputation using mean of feature values."],"metadata":{"id":"aVHttW1C99R3"}},{"cell_type":"code","source":["#importing the required libraries\n","import numpy as np\n","from sklearn.impute import SimpleImputer\n","# defining the input 2 dimensional list (each internal list shows feature values of a datapoint)\n","X = [[5, 1, 2, 8],\n","     [2, 3, np.nan, 4],\n","     [5, 4, 4, 6],\n","     [8, 5, np.nan, 7],\n","     [7, 8, 8, 3]]\n","# fit a SimpleImputer function by defining what to be considered as missing value and what strategy to be used for imputation\n","# strategy options: mean, median, most_frequent, constant\n","imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n","# fitting the model\n","imp.fit(X)\n","# calculate missing values of the input\n","X_no_missing = imp.transform(X)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KWwcUT5kkGf9","executionInfo":{"status":"ok","timestamp":1668906530958,"user_tz":300,"elapsed":130,"user":{"displayName":"Ali Madani","userId":"16080091956158850791"}},"outputId":"06fcfcb9-3d54-4b1d-9943-51274f4c8f0a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[5.         1.         2.         8.        ]\n"," [2.         3.         4.66666667 4.        ]\n"," [5.         4.         4.         6.        ]\n"," [8.         5.         4.66666667 7.        ]\n"," [7.         8.         8.         3.        ]]\n"]}]},{"cell_type":"markdown","source":["The following code focuses on feature imputation using linear regression to calculate missing values of one feature, that is feature 3, using its highest correlated feature with low or no missing values, that is feature 2 in this example.\n"],"metadata":{"id":"MqPXZvYTJBb8"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fsrFV4OXnlGZ","executionInfo":{"status":"ok","timestamp":1668890526404,"user_tz":300,"elapsed":335,"user":{"displayName":"Ali Madani","userId":"16080091956158850791"}},"outputId":"6eea4022-9619-47b8-d4ef-efb0350d4eba"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["LinearRegression()"]},"metadata":{},"execution_count":2}],"source":["#importing the required libraries\n","import numpy as np\n","from sklearn.linear_model import LinearRegression as LR\n","\n","# defining input variables for feature 2 and 3\n","f2 = np.array([1, 4, 8]).reshape((-1, 1))\n","f3 = np.array([2, 4, 8])\n","# initializing a linear regression model with sklearn LinearRegression\n","model = LR()\n","# fitting the linear regression model using f2 and f3 as input and output variables, respectively\n","model.fit(f2, f3)\n","# predicting missing values of feature 3\n","model.predict(np.array([3, 5]).reshape((-1, 1)))"]}]}